{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5QkshukqOSPF",
        "outputId": "20c67186-c5c4-4ad6-b518-16fe9e8a11d9"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────\n",
        "#          PySpark CSV + Common Operations\n",
        "# ───────────────────────────────────────────────\n",
        "\n",
        "# === 1. Basic session creation (Spark 3.x ) =============================\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV_cheatsheet_example\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# sc = spark.sparkContext\n",
        "\n",
        "# === 2. Reading CSV files ===============================================\n",
        "df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "# or more explicit / safer version:\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"encoding\", \"UTF-8\") \\\n",
        "    .option(\"escape\", '\"') \\\n",
        "    .option(\"quote\", '\"') \\\n",
        "    .option(\"mode\", \"PERMISSIVE\") \\\n",
        "    .csv(\"data/*.csv\")\n",
        "\n",
        "# very common modern pattern (especially 2024–2026):\n",
        "df = spark.read.option(\"header\",True).csv(\"s3://bucket/folder/*.csv.gz\")\n",
        "\n",
        "# === 3. Quick inspection ================================================\n",
        "df.printSchema()                                             # shows column names + inferred types\n",
        "df.show(5, truncate=40, vertical=False)                      # vertical=True is great for wide tables\n",
        "df.select(\"*\").limit(20).toPandas()                          # careful – only for small results!\n",
        "\n",
        "# get row count (action!)\n",
        "df.count()\n",
        "\n",
        "# === 4. Most useful column operations ===================================\n",
        "from pyspark.sql.functions import col, column\n",
        "\n",
        "# Select & rename\n",
        "df.select(\"id\", \"name\", col(\"salary\").alias(\"monthly_salary\"))\n",
        "\n",
        "# Filter (two equivalent styles)\n",
        "df.filter(\"age > 30 AND salary < 80000\")\n",
        "df.filter((col(\"age\") > 30) & (col(\"salary\") < 80000))\n",
        "\n",
        "# Add / replace column\n",
        "from pyspark.sql.functions import lit, when, concat_ws, lower, upper\n",
        "\n",
        "df = df.withColumn(\"country\", lit(\"Kenya\"))                  # constant value\n",
        "df = df.withColumn(\"senior\", when(col(\"age\") >= 35, True).otherwise(False))\n",
        "df = df.withColumn(\"full_name\", concat_ws(\" \", \"first_name\", \"last_name\"))\n",
        "df = df.withColumn(\"email_lower\", lower(col(\"email\")))\n",
        "\n",
        "# === 5. Handling nulls / missing values ================================\n",
        "from pyspark.sql.functions import coalesce, isnan, when, count\n",
        "\n",
        "df = df.na.fill({\"salary\": 0, \"age\": -1})                    # fill nulls with specific values\n",
        "df = df.na.drop(\"any\")                                       # drop row if ANY column is null\n",
        "df = df.na.drop(\"all\", subset=[\"email\", \"phone\"])            # drop only if ALL listed cols null\n",
        "\n",
        "# count nulls per column (very useful pattern)\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# === 6. GroupBy + Aggregations (most frequent task) ====================\n",
        "from pyspark.sql.functions import count, sum, avg, min, max, countDistinct\n",
        "\n",
        "result = df.groupBy(\"department\", \"city\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"headcount\"),\n",
        "        countDistinct(\"employee_id\").alias(\"unique_employees\"),\n",
        "        sum(\"salary\").alias(\"total_salary\"),\n",
        "        avg(\"salary\").alias(\"avg_salary\"),\n",
        "        max(\"age\").alias(\"oldest\")\n",
        "    ) \\\n",
        "    .orderBy(\"total_salary\", ascending=False)\n",
        "\n",
        "result.show(truncate=False)\n",
        "\n",
        "# withColumn + agg pattern (very common)\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "df.groupBy(\"department\").agg(\n",
        "    round(avg(\"salary\"), 0).alias(\"avg_salary_round\"),\n",
        "    (sum(\"salary\") / 1000000).alias(\"salary_millions\")\n",
        ").orderBy(\"avg_salary_round\", ascending=False)\n",
        "\n",
        "# === 7. Joins (most common types) =======================================\n",
        "orders = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "customers = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# inner (default)\n",
        "df_joined = orders.join(customers, \"customer_id\", \"inner\")\n",
        "\n",
        "# left / right / full / anti / cross\n",
        "df_left  = orders.join(customers, \"customer_id\", \"left\")\n",
        "df_anti  = orders.join(customers, \"customer_id\", \"left_anti\")   # rows in orders without match\n",
        "\n",
        "# multi-column join\n",
        "# df.join(other_df,\n",
        "#         (df.customer_id == other_df.id) & (df.country == other_df.country),\n",
        "#         \"left\")\n",
        "\n",
        "# === 8. Window functions (ranking, running totals, etc) ================\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
        "\n",
        "df_with_rank = df.withColumn(\"rank\", F.rank().over(window_spec)) \\\n",
        "                 .withColumn(\"dense_rank\", F.dense_rank().over(window_spec)) \\\n",
        "                 .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
        "\n",
        "# running total example\n",
        "window_cum = Window.partitionBy(\"department\").orderBy(\"hire_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df = df.withColumn(\"cumulative_salary\", F.sum(\"salary\").over(window_cum))\n",
        "\n",
        "# === 9. Writing results =================================================\n",
        "# most common formats\n",
        "df.write.mode(\"overwrite\").parquet(\"s3://bucket/results/employees.parquet/\")\n",
        "df.write.mode(\"append\").partitionBy(\"year\",\"month\").parquet(\"output/\")\n",
        "\n",
        "# CSV output (less common in big data, but still used)\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"compression\", \"gzip\") \\\n",
        "    .csv(\"output/my_result_csv/\")\n",
        "\n",
        "# single file output (small data only!)\n",
        "df.coalesce(1).write.mode(\"overwrite\").csv(\"small_result/\", header=True)\n",
        "\n",
        "# === 10. Quick one-liners you use all the time ========================\n",
        "df.cache()                              # or .persist() — very important for iterative work\n",
        "df.unpersist()                          # free memory\n",
        "\n",
        "df.createOrReplaceTempView(\"employees\") # then use SQL\n",
        "spark.sql(\"SELECT department, AVG(salary) FROM employees GROUP BY department\")\n",
        "\n",
        "df.explain()                            # see physical + logical plan\n",
        "df.explain(\"extended\")                  # more detailed\n",
        "\n",
        "# Stop session when finished\n",
        "spark.stop()                          # usually done automatically in notebooks"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
